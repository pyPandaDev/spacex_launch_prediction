{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– SpaceX Launch Success Prediction - Model Training\n",
    "\n",
    "## Objectives:\n",
    "1. Prepare data for modeling\n",
    "2. Train baseline models\n",
    "3. Evaluate and compare models\n",
    "4. Perform hyperparameter tuning\n",
    "5. Analyze feature importance\n",
    "6. Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_preprocessing import prepare_for_modeling, get_feature_target_split\n",
    "from model_training import *\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data\n",
    "df = pd.read_csv('../data/spacex_cleaned.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Success rate: {df['Mission_Success'].mean():.2%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "df_model = prepare_for_modeling(df)\n",
    "print(f\"Model dataset shape: {df_model.shape}\")\n",
    "print(f\"\\nFeatures: {df_model.drop(columns='Mission_Success').columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X, y = get_feature_target_split(df_model)\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain success rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test success rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "print(\"Training baseline models...\\n\")\n",
    "models = train_baseline_models(X_train, y_train)\n",
    "print(\"\\nâœ“ All models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = evaluate_models(models, X_test, y_test)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "results_melted = results.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "sns.barplot(data=results_melted, x='Metric', y='Score', hue='Model', ax=ax)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig = plot_roc_curve(models, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Failure', 'Success'],\n",
    "                yticklabels=['Failure', 'Success'])\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for each model\n",
    "print(\"Performing 5-Fold Cross-Validation...\\n\")\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_result = cross_validate_model(model, X, y, cv=5)\n",
    "    cv_results[name] = cv_result\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {cv_result['mean_accuracy']:.4f} (+/- {cv_result['std_accuracy']:.4f})\")\n",
    "    print(f\"  Individual Folds: {cv_result['scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Random Forest\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_best, rf_params = tune_random_forest(X_train, y_train)\n",
    "print(f\"\\nBest parameters: {rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune XGBoost\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_best, xgb_params = tune_xgboost(X_train, y_train)\n",
    "print(f\"\\nBest parameters: {xgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models\n",
    "tuned_models = {\n",
    "    'Random Forest (Tuned)': rf_best,\n",
    "    'XGBoost (Tuned)': xgb_best\n",
    "}\n",
    "\n",
    "tuned_results = evaluate_models(tuned_models, X_test, y_test)\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "print(\"=\" * 80)\n",
    "print(tuned_results.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "fig = plot_feature_importance(rf_best, X.columns, top_n=15)\n",
    "if fig:\n",
    "    plt.title('Random Forest - Feature Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for XGBoost\n",
    "fig = plot_feature_importance(xgb_best, X.columns, top_n=15)\n",
    "if fig:\n",
    "    plt.title('XGBoost - Feature Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Select Best Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on ROC-AUC\n",
    "all_results = pd.concat([results, tuned_results], ignore_index=True)\n",
    "best_model_name = all_results.loc[all_results['ROC-AUC'].idxmax(), 'Model']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {all_results['ROC-AUC'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model\n",
    "if 'Tuned' in best_model_name:\n",
    "    best_model = tuned_models[best_model_name]\n",
    "else:\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "# Get comprehensive report\n",
    "report = get_model_report(best_model, X_test, y_test)\n",
    "print(\"\\nBest Model Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "print(report['classification_report'])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "model_path = '../models/best_model.pkl'\n",
    "save_model(best_model, model_path)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "joblib.dump(feature_names, '../models/feature_names.pkl')\n",
    "print(f\"Feature names saved to ../models/feature_names.pkl\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': report['accuracy'],\n",
    "    'precision': report['precision'],\n",
    "    'recall': report['recall'],\n",
    "    'f1': report['f1'],\n",
    "    'roc_auc': report.get('roc_auc', None),\n",
    "    'features': feature_names\n",
    "}\n",
    "joblib.dump(metadata, '../models/model_metadata.pkl')\n",
    "print(f\"Model metadata saved to ../models/model_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Summary\n",
    "\n",
    "### Best Model Performance:\n",
    "- The model has been trained and evaluated successfully\n",
    "- Feature importance has been analyzed\n",
    "- The model is saved and ready for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
